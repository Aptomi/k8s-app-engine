kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ printf "spark-conf-%s" .Release.Name | trunc 55 | trimSuffix "-" }}
  labels:
    heritage: {{ .Release.Service | quote }}
    release: {{ .Release.Name | quote }}
    chart: {{ .Chart.Name }}-{{ .Chart.Version }}
data:
  {{ if .Values.configs.sparkDefaults }}
  spark-defaults.conf: |+
    # spark-defaults.conf configs
    {{ .Values.configs.sparkDefaults | indent 4 | trim }}
  {{ end }}
  spark-env.sh: |+
    # spark-env.sh configs
    SPARK_DAEMON_MEMORY={{ $.Values.spark.master.daemonMemory | quote }}
    SPARK_NO_DAEMONIZE=1
    {{ if ne (int .Values.spark.master.replicas) 1 }}
    SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.dir={{ .Values.zookeeper.path }} -Dspark.deploy.zookeeper.url={{ template "zk-address" . -}}"
    {{ end }}
    {{ if .Values.prometheusExporter.enabled }}
    SPARK_DAEMON_JAVA_OPTS="${SPARK_DAEMON_JAVA_OPTS} -javaagent:/usr/local/spark/jmx_prometheus_javaagent-0.9.jar=7072:/usr/local/spark/conf/spark-jmx.yml"
    {{ end }}
  {{ if .Values.configs.sparkEnv }}
    {{ .Values.configs.sparkEnv | indent 4 | trim }}
  {{ end }}
  log4j.properties: |-
    log4j.rootLogger=INFO, file, console

    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d [%t] %-5p %c - %m%n

    log4j.appender.file=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.file.File=/var/log/spark/spark.log
    log4j.appender.file.DatePattern='.'yyyy-MM-dd
    log4j.appender.file.layout=org.apache.log4j.PatternLayout
    log4j.appender.file.layout.ConversionPattern=[%p] %d %c %M - %m%n

  {{- if .Values.prometheusExporter.enabled }}
  spark-jmx.yml: |-
   rules:
     # These come from the master
     # Example: master.aliveWorkers
     - pattern: "metrics<name=master.(.*)><>Value"
       name: spark_master_$1

     # These come from the worker
     # Example: worker.coresFree
     - pattern: "metrics<name=worker.(.*)><>Value"
       name: spark_worker_$1

     # These come from the application driver
     # Example: app-20160809000059-0000.driver.DAGScheduler.stage.failedStages
     - pattern: "metrics<name=(.*).driver.(DAGScheduler|BlockManager).(.*)><>Value"
       name: spark_driver_$2_$3
       labels:
         app_id: "$1"

     # These come from the application driver if it's a streaming application
     # Example: app-20160809000059-0000.driver.com.example.ClassName.StreamingMetrics.streaming.lastCompletedBatch_schedulingDelay
     - pattern: "metrics<name=(.*).driver.(.*).StreamingMetrics.streaming.(.*)><>Value"
       name: spark_streaming_driver_$3
       labels:
         app_id: "$1"
         app_name: "$2"

     # These come from the application executors
     # Example: app-20160809000059-0000.0.executor.threadpool.activeTasks
     - pattern: "metrics<name=(.*)\\.(.*).executor.(.*)><>Value"
       name: spark_executor_$3
       labels:
         app_id: "$1"
         executor_id: "$2"

     # These come from the master
     # Example: application.com.example.ClassName.1470700859054.cores
     - pattern: "metrics<name=application.(.*)\\.([0-9]+)\\.(.*)><>Value"
       name: spark_application_$3
       labels:
         app_name: "$1"
         app_start_epoch: "$2"

  metrics.properties: |-
    *.sink.jmx.class=org.apache.spark.metrics.sink.JmxSink
  {{- end }}
